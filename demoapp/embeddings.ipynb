{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47883cf9",
   "metadata": {},
   "source": [
    "# pdf íŒŒì¼ json íŒŒì¼ë¡œ ë³€í™˜ (í•œêµ­ì–´ ì‚¬ì „/ì˜ì–´ ì‚¬ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94269540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-6.6.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pypdf-6.6.2-py3-none-any.whl (329 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-6.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69264cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF ë‚´ìš©ì„ ì½ì–´ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ì•½ 1ë¶„ ì†Œìš”)\n",
      "ë³€í™˜ ì™„ë£Œ! ì´ 3126ê°œì˜ ìš©ì–´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# 1. PDF íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš” (íŒŒì¼ì´ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤)\n",
    "pdf_filename = \"data/ì§€ì‹ì¬ì‚°ê¶Œìš©ì–´ì‚¬ì „_í¸ì§‘ë³¸_v16.pdf\"\n",
    "\n",
    "def create_json_dictionaries(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ì˜¤ë¥˜: '{pdf_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    full_text = \"\"\n",
    "    print(\"PDF ë‚´ìš©ì„ ì½ì–´ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ì•½ 1ë¶„ ì†Œìš”)\")\n",
    "    \n",
    "    # ì „ì²´ í˜ì´ì§€ ìŠ¤ìº” (ëª©ì°¨ ì œì™¸)\n",
    "    for i in range(6, 301):\n",
    "        if i >= len(reader.pages): break\n",
    "        page = reader.pages[i]\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        filtered_lines = []\n",
    "        for line in lines:\n",
    "            # ë¶ˆí•„ìš”í•œ í—¤ë”/í‘¸í„° ì œê±°\n",
    "            if re.search(r'^\\d+\\s+\\d+', line): continue \n",
    "            if re.search(r'^\\d+\\s+ì§€ì‹ì¬ì‚°ê¶Œ', line): continue\n",
    "            if re.search(r'ì˜í•œì¤‘ì‚¬ì „\\s+\\d+', line): continue\n",
    "            if re.search(r'í•œì˜ì¤‘ì‚¬ì „\\s+\\d+', line): continue\n",
    "            if re.search(r'^\\s*[A-Zã„±-ã…]\\s*$', line): continue\n",
    "            if \"ì§€ì‹ì¬ì‚°ê¶Œ ìš©ì–´ ì‚¬ì „\" in line: continue\n",
    "            if \"AA\" == line.strip(): continue # ë…¸ì´ì¦ˆ ì œê±°\n",
    "            filtered_lines.append(line.strip())\n",
    "        full_text += \"\\n\".join(filtered_lines) + \"\\n\"\n",
    "\n",
    "    all_lines = [l for l in full_text.split('\\n') if l]\n",
    "    entries = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(all_lines) - 1:\n",
    "        line = all_lines[i]\n",
    "        next_line = all_lines[i+1]\n",
    "        \n",
    "        # ì˜ì–´ -> í•œêµ­ì–´ íŒ¨í„´\n",
    "        is_eng_line = re.match(r'^[A-Za-z]', line) and not re.search(r'[ê°€-í£]', line)\n",
    "        next_has_kor = re.search(r'[ê°€-í£]', next_line)\n",
    "        \n",
    "        # í•œêµ­ì–´ -> ì˜ì–´ íŒ¨í„´\n",
    "        is_kor_line = re.search(r'[ê°€-í£]', line) and len(line) < 60\n",
    "        next_starts_eng = re.match(r'^[A-Za-z]', next_line)\n",
    "        \n",
    "        if is_eng_line and next_has_kor:\n",
    "            eng_term = line\n",
    "            kor_term_raw = next_line\n",
    "            # ì¤‘êµ­ì–´ ì œê±°\n",
    "            parts = re.split(r'\\s{2,}|\\t|\\u2002', kor_term_raw)\n",
    "            kor_term = parts[0].strip()\n",
    "            kor_term = re.sub(r'[\\u4e00-\\u9fff]+', '', kor_term).strip()\n",
    "            entries.append({'eng': eng_term, 'kor': kor_term, 'desc_lines': []})\n",
    "            i += 2\n",
    "            \n",
    "        elif is_kor_line and next_starts_eng:\n",
    "            kor_term = line\n",
    "            eng_term_raw = next_line\n",
    "            parts = re.split(r'\\s{2,}|\\t|\\u2002', eng_term_raw)\n",
    "            eng_term = parts[0].strip()\n",
    "            eng_term = re.sub(r'[\\u4e00-\\u9fff]+', '', eng_term).strip()\n",
    "            entries.append({'eng': eng_term, 'kor': kor_term, 'desc_lines': []})\n",
    "            i += 2\n",
    "        else:\n",
    "            if entries and not (re.search(r'\\s+\\d+$', line) and len(line) < 40):\n",
    "                entries[-1]['desc_lines'].append(line)\n",
    "            i += 1\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬\n",
    "    clean_entries = []\n",
    "    seen = set()\n",
    "    for entry in entries:\n",
    "        desc = \" \".join(entry['desc_lines']).strip()\n",
    "        desc = re.sub(r'\\s+', ' ', desc)\n",
    "        key = (entry['eng'].lower().strip(), entry['kor'].strip())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            clean_entries.append({\"eng\": entry['eng'], \"kor\": entry['kor'], \"desc\": desc})\n",
    "\n",
    "    # íŒŒì¼ 1: í•œêµ­ì–´ ì¤‘ì‹¬ JSON\n",
    "    kor_json = [{\"ë‹¨ì–´\": x['kor'], \"ì„¤ëª…\": x['desc'], \"ì˜ì–´ëŒ€ì—­ì–´\": x['eng']} for x in clean_entries]\n",
    "    \n",
    "    # íŒŒì¼ 2: ì˜ì–´ ì¤‘ì‹¬ JSON\n",
    "    eng_json = [{\"word\": x['eng'], \"description\": x['desc'], \"korean_equivalent\": x['kor']} for x in clean_entries]\n",
    "\n",
    "    with open(\"korean_patent_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(kor_json, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    with open(\"english_patent_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eng_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"ë³€í™˜ ì™„ë£Œ! ì´ {len(clean_entries)}ê°œì˜ ìš©ì–´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    create_json_dictionaries(pdf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302dd369",
   "metadata": {},
   "source": [
    "# 3230ê°œ ë‹¨ì–´ê°€ ë§ëŠ”ì§€ í™•ì¸ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f5153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF ë‚´ìš©ì„ ìŠ¤ìº”í•˜ì—¬ ë‹¨ì–´ ìˆ˜ë¥¼ ê³„ì‚° ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n",
      "------------------------------\n",
      "ê²€ì¦ ê²°ê³¼: ì´ 3126ê°œì˜ ê³ ìœ  ë‹¨ì–´ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ê²€ì¦ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "\n",
    "\n",
    "import re\n",
    "# !pip install pypdf  # pypdfê°€ ì—†ë‹¤ë©´ ì´ ì¤„ì˜ ì£¼ì„(#)ì„ ì§€ìš°ê³  ë¨¼ì € ì„¤ì¹˜í•˜ì„¸ìš”\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì—…ë¡œë“œí•œ íŒŒì¼ëª…ê³¼ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤)\n",
    "file_path = \"data/ì§€ì‹ì¬ì‚°ê¶Œìš©ì–´ì‚¬ì „_í¸ì§‘ë³¸_v16.pdf\"\n",
    "\n",
    "# 2. í•¨ìˆ˜ ì •ì˜ (ì´ ë¶€ë¶„ì´ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤)\n",
    "def verify_word_count(pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        return f\"ì˜¤ë¥˜: '{pdf_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°™ì€ í´ë”ì— íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "    except Exception as e:\n",
    "        return f\"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "    full_text = \"\"\n",
    "    print(\"PDF ë‚´ìš©ì„ ìŠ¤ìº”í•˜ì—¬ ë‹¨ì–´ ìˆ˜ë¥¼ ê³„ì‚° ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
    "\n",
    "    # ì‚¬ì „ ë³¸ë¬¸ ë²”ìœ„ (ëª©ì°¨ì™€ ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•œ 6~300í˜ì´ì§€)\n",
    "    for i in range(6, 301):\n",
    "        if i >= len(reader.pages): break\n",
    "        page = reader.pages[i]\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ ë…¸ì´ì¦ˆ(í—¤ë”, í˜ì´ì§€ë²ˆí˜¸ ë“±) ì œê±°\n",
    "        lines = text.split('\\n')\n",
    "        filtered_lines = []\n",
    "        for line in lines:\n",
    "            if re.search(r'^\\d+\\s+\\d+', line): continue \n",
    "            if re.search(r'^\\d+\\s+ì§€ì‹ì¬ì‚°ê¶Œ', line): continue\n",
    "            if re.search(r'ì˜í•œì¤‘ì‚¬ì „\\s+\\d+', line): continue\n",
    "            if re.search(r'í•œì˜ì¤‘ì‚¬ì „\\s+\\d+', line): continue\n",
    "            if re.search(r'^\\s*[A-Zã„±-ã…]\\s*$', line): continue\n",
    "            if \"ì§€ì‹ì¬ì‚°ê¶Œ ìš©ì–´ ì‚¬ì „\" in line: continue\n",
    "            if \"AA\" == line.strip(): continue\n",
    "            if \"ì°¾ì•„ë³´ê¸°\" in line: continue\n",
    "            filtered_lines.append(line.strip())\n",
    "        full_text += \"\\n\".join(filtered_lines) + \"\\n\"\n",
    "\n",
    "    all_lines = [l for l in full_text.split('\\n') if l]\n",
    "    entries = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(all_lines) - 1:\n",
    "        line = all_lines[i]\n",
    "        next_line = all_lines[i+1]\n",
    "        \n",
    "        # [íŒ¨í„´ 1] ì˜ì–´ ë‹¨ì–´ -> í•œêµ­ì–´ ë‹¨ì–´ (ì˜-í•œ ì„¹ì…˜)\n",
    "        is_eng_line = re.match(r'^[A-Za-z]', line) and not re.search(r'[ê°€-í£]', line)\n",
    "        next_has_kor = re.search(r'[ê°€-í£]', next_line)\n",
    "        \n",
    "        # [íŒ¨í„´ 2] í•œêµ­ì–´ ë‹¨ì–´ -> ì˜ì–´ ë‹¨ì–´ (í•œ-ì˜ ì„¹ì…˜)\n",
    "        is_kor_line = re.search(r'[ê°€-í£]', line) and len(line) < 60\n",
    "        next_starts_eng = re.match(r'^[A-Za-z]', next_line)\n",
    "        \n",
    "        if is_eng_line and next_has_kor:\n",
    "            eng_term = line.strip()\n",
    "            kor_term_raw = next_line\n",
    "            parts = re.split(r'\\s{2,}|\\t|\\u2002', kor_term_raw)\n",
    "            kor_term = parts[0].strip()\n",
    "            kor_term = re.sub(r'[\\u4e00-\\u9fff]+', '', kor_term).strip()\n",
    "            entries.append((eng_term, kor_term))\n",
    "            i += 2\n",
    "            \n",
    "        elif is_kor_line and next_starts_eng:\n",
    "            kor_term = line.strip()\n",
    "            eng_term_raw = next_line\n",
    "            parts = re.split(r'\\s{2,}|\\t|\\u2002', eng_term_raw)\n",
    "            eng_term = parts[0].strip()\n",
    "            eng_term = re.sub(r'[\\u4e00-\\u9fff]+', '', eng_term).strip()\n",
    "            entries.append((eng_term, kor_term))\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±° (ì˜ì–´ ì†Œë¬¸ì + í•œêµ­ì–´ ë‹¨ì–´ ê¸°ì¤€)\n",
    "    unique_entries = set()\n",
    "    for eng, kor in entries:\n",
    "        key = (eng.lower().strip(), kor.strip())\n",
    "        unique_entries.add(key)\n",
    "\n",
    "    return len(unique_entries)\n",
    "\n",
    "# 3. í•¨ìˆ˜ ì‹¤í–‰ (ì •ì˜ê°€ ì™„ë£Œëœ í›„ ì‹¤í–‰ë©ë‹ˆë‹¤)\n",
    "count = verify_word_count(file_path)\n",
    "print(\"-\" * 30)\n",
    "print(f\"ê²€ì¦ ê²°ê³¼: ì´ {count}ê°œì˜ ê³ ìœ  ë‹¨ì–´ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a225b5a",
   "metadata": {},
   "source": [
    "# í•„ìš” ì—†ëŠ” ë‹¨ì–´ ì‚­ì œ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f47136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ì˜¤ë¥˜: ì›ë³¸ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "----------------------------------------\n",
      "âœ… ì •ì œ ë° ì €ì¥ ì™„ë£Œ!\n",
      "   - ì €ì¥ëœ íŒŒì¼: korean_patent_dict_clean.json, english_patent_dict_clean.json\n",
      "----------------------------------------\n",
      "ğŸ“Š [ìµœì¢… ë‹¨ì–´ ìˆ˜ í™•ì¸]\n",
      "   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì‚¬ì „: ì´ 0ê°œ (ì‚­ì œëœ ë…¸ì´ì¦ˆ: 0ê°œ)\n",
      "   ğŸ‡ºğŸ‡¸ ì˜ì–´ ì‚¬ì „:   ì´ 0ê°œ (ì‚­ì œëœ ë…¸ì´ì¦ˆ: 0ê°œ)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# 1. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ì•ì„œ ë§Œë“  íŒŒì¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)\n",
    "input_kor = 'korean_patent_dict.json'\n",
    "input_eng = 'english_patent_dict.json'\n",
    "\n",
    "output_kor = 'korean_patent_dict_clean.json'\n",
    "output_eng = 'english_patent_dict_clean.json'\n",
    "\n",
    "try:\n",
    "    with open(input_kor, 'r', encoding='utf-8') as f:\n",
    "        kor_data = json.load(f)\n",
    "    with open(input_eng, 'r', encoding='utf-8') as f:\n",
    "        eng_data = json.load(f)\n",
    "    print(f\"ğŸ“‚ ì›ë³¸ íŒŒì¼ ë¡œë“œ ì„±ê³µ\")\n",
    "    print(f\"   - í•œêµ­ì–´ ì›ë³¸: {len(kor_data)}ê°œ\")\n",
    "    print(f\"   - ì˜ì–´ ì›ë³¸: {len(eng_data)}ê°œ\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ì˜¤ë¥˜: ì›ë³¸ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    kor_data, eng_data = [], []\n",
    "\n",
    "# 2. í•„í„°ë§ í•¨ìˆ˜ (ë…¸ì´ì¦ˆ ì œê±° ê·œì¹™)\n",
    "def is_valid_word(word):\n",
    "    # ë¹ˆ ë¬¸ìì—´ ì œê±°\n",
    "    if not word or not word.strip():\n",
    "        return False\n",
    "    # íŠ¹ìˆ˜ê¸°í˜¸/ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œê±° (í•œê¸€/ì˜ì–´ê°€ ìµœì†Œ 1ì í¬í•¨ë˜ì–´ì•¼ í•¨)\n",
    "    if not re.search(r'[ê°€-í£a-zA-Z]', word):\n",
    "        return False\n",
    "    # ê¸¸ì´ê°€ 1ê¸€ìì´ë©´ì„œ ì•ŒíŒŒë²³ ëŒ€ë¬¸ìë‚˜ ììŒì¸ ê²½ìš° ì œê±° (ìƒ‰ì¸ ê¸°í˜¸ ë“±)\n",
    "    if len(word) == 1 and re.match(r'[A-Zã„±-ã…]', word):\n",
    "        return False\n",
    "    # íŠ¹ì • ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ ì œê±°\n",
    "    noise_keywords = [\"AA\", \"ì˜â€¤í•œâ€¤ì¤‘\", \"ì§€ì‹ì¬ì‚°ê¶Œ ìš©ì–´ ì‚¬ì „\", \"í•œê¸€ë¡œ ì°¾ê¸°\", \"ì˜ì–´ë¡œ ì°¾ê¸°\", \"ì°¾ì•„ë³´ê¸°\"]\n",
    "    if word in noise_keywords:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 3. ë°ì´í„° ì •ì œ ì‹¤í–‰\n",
    "clean_kor_data = [item for item in kor_data if is_valid_word(item['ë‹¨ì–´'])]\n",
    "clean_eng_data = [item for item in eng_data if is_valid_word(item['word'])]\n",
    "\n",
    "# 4. ê²°ê³¼ ì €ì¥\n",
    "with open(output_kor, 'w', encoding='utf-8') as f:\n",
    "    json.dump(clean_kor_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(output_eng, 'w', encoding='utf-8') as f:\n",
    "    json.dump(clean_eng_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 5. ìµœì¢… ê°œìˆ˜ í™•ì¸ ë° ë¦¬í¬íŠ¸\n",
    "print(\"-\" * 40)\n",
    "print(f\"âœ… ì •ì œ ë° ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"   - ì €ì¥ëœ íŒŒì¼: {output_kor}, {output_eng}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"ğŸ“Š [ìµœì¢… ë‹¨ì–´ ìˆ˜ í™•ì¸]\")\n",
    "print(f\"   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì‚¬ì „: ì´ {len(clean_kor_data)}ê°œ (ì‚­ì œëœ ë…¸ì´ì¦ˆ: {len(kor_data) - len(clean_kor_data)}ê°œ)\")\n",
    "print(f\"   ğŸ‡ºğŸ‡¸ ì˜ì–´ ì‚¬ì „:   ì´ {len(clean_eng_data)}ê°œ (ì‚­ì œëœ ë…¸ì´ì¦ˆ: {len(eng_data) - len(clean_eng_data)}ê°œ)\")\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_basic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
